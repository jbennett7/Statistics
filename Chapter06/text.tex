% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{epsdice}
\usepackage{pst-poker}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Elementary Probability Theory},
  pdfauthor={Joseph Bennett},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Elementary Probability Theory}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Text}
\author{Joseph Bennett}
\date{2024-11-02}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{definitions-of-probability}{%
\section{Definitions of probability}\label{definitions-of-probability}}

\hypertarget{classic-definition}{%
\subsection{Classic definition}\label{classic-definition}}

Suppose that an event \(E\) can happen in \(h\) ways out of a total of
\(n\) possible equally likely ways. Then the probability of occurrence
of the event (called its \emph{success}) is denoted by

\[
p = \text{Pr}\{E\} = \frac{h}{n}
\]

The probability of nonoccurrence of the event (called its
\emph{failure}) is denoted by

\[
q = \text{Pr}\{\text{not } E\} = \frac{n - h}{n} = 1 - \frac{h}{n} = 1 - p
  = 1 - \text{Pr}\{E\}
\]

Thus \(p + q = 1\), or
\(\text{Pr}\{E\} + \text{Pr}\{\text{not } E\} = 1\). The event ``not
\(E\)'' is sometimes denoted by \(\bar{E}, \tilde{E},\) or \(\sim E\).

\textbf{Example 1.} When a die is tossed, there are 6 equally possible
ways in which the die can fall:

\[
\scalebox{4}{
\epsdice{1} \epsdice{2} \epsdice{3} \epsdice{4} \epsdice{5} \epsdice{6}
}
\]

The event \(E\), that a 3 or 4 turns up, is:

\[
\scalebox{4}{
\epsdice{3} \epsdice{4}
}
\]

and the probability of \(E\) is \(\text{Pr}\{E\} = 2/6\) or \(1/3\). The
probability of not getting a 3 or 4 (i.e., getting a 1, 2, 5, or 6) is
\(\text{Pr}\{\bar{E}\} = 1 - \text{Pr}\{E\} = 2/3\).

Note that the probability of an event is a number between 0 and 1. If
the event cannot occur, its probability is 0. If it must occur (i.e.,
its occurrence is \emph{certain}), its probability is 1.

If \(p\) is the probability that an event will occur, the \emph{odds} in
favor of its happening are \(p : q\) (read ``\(p\) to \(q\)''); the odds
agains its happening are \(q : p\). Thus the odds against a 3 or 4 in a
single toss of a fair die are
\(q : p = \frac{2}{3} : \frac{1}{3} = 2 : 1\) (i.e., 2 to 1).

\hypertarget{relative-frequency-definition}{%
\subsection{Relative-frequency
definition}\label{relative-frequency-definition}}

The classic definition of probability has a disadvantage in that the
words ``equally likely'' are vague. In fact, since these words seem to
be synonymous with ``equally probable,'' the definition is
\emph{circular} because we are essentially defining probability in terms
of itself. For this reason, a statistical definition of probability has
been advocated by some people. According to this the estimated
probability, or \emph{empirical probability}, of an event is taken to be
the \emph{relative frequency} of occurrence of the event when the number
of observations is very large. The probability itself is the
\emph{limit} of the relative frequency as the number of observations
increases indefinitely.

\textbf{Example 2.} If 1000 tosses of a coin results in 529 heads, the
relative frequency of heads is \(529/100 = 0.529\). If another 1000
tosses results in 493 heads, the relative frequency in the total of 2000
tosses is \((529 + 493)/2000 = 0.511\). According to the statistical
definition, by continuing in this manner we should ultimately get closer
and closer to a number that represents the probability of a head in a
single toss of the coin. From the results so far presented, this should
be \(0.5\) to one significant figure. To obtain more significant
figures, further observations must be made.

The statistical definition, although useful in practice, has
difficulties from a mathematical point of view, since an actual limiting
number may not really exist. For this reason, modern probability theory
has been developed \emph{axiomatically}; that is, the theory leaves the
concept of probability undefined, much the same as \emph{point} and
\emph{line} are undefined in geometry.

\hypertarget{conditional-probability-independent-and-dependent-events}{%
\section{Conditional probability: Independent and dependent
events}\label{conditional-probability-independent-and-dependent-events}}

If \(E_1\) and \(E_2\) are two events, the probability that \(E_2\)
occurs given that \(E_1\) has occurred is denoted by
\(\text{Pr}\{E_2|E_1\}\), or \(\text{Pr}\{E_2 \text{ given } E_1\}\),
and is called the \emph{conditional probability} of \(E_2\) given that
\(E_1\) has occurred.

If the occurrence or nonoccurrence of \(E_1\) does not affect the
probability of occurrence of \(E_2\), then
\(\text{Pr}\{E_2|E_1\} = \text{Pr}\{E_2\}\) and we say that \(E_1\) and
\(E_2\) are \emph{independent events}; otherwise, they are
\emph{dependent events}.

If we denote by \(E_1E_2\) the event that ``both \(E_1\) and \(E_2\)
occur,'' sometimes called a \emph{compound event}, then

\begin{equation}
\text{Pr}\{E_1E_2\} = \text{Pr}\{E_1\}\ \text{Pr}\{E_2|E_1\}
\end{equation}

In particular,

\begin{equation}
\text{Pr}\{E_1E_2\} = \text{Pr}\{E_1\}\ \text{Pr}\{E_2\}
\ \ \ \ \text{ for independent events}
\end{equation}

For three events \(E_1\), \(E_2\), and \(E_3\), we have

\begin{equation}
\text{Pr}\{E_1 E_2 E_3\} = \text{Pr}\{E_1\}\ \text{Pr}\{E_2|E_1\}
\ \text{Pr}\{E_3|E_1 E_2\}
\end{equation}

That is, the probability of occurrence of \(E_1\), \(E_2\), and \(E_3\)
is equal to (the probability of \(E_1\)) \times (the probability of
\(E_2\) given that \(E_1\) has occurred) \times (the probabilty of
\(E_3\) given that both \(E_1\) and \(E_2\) have occurred). In
particular,

\begin{equation}
\text{Pr}\{E_1 E_2 E_3\} = \text{Pr}\{E_1\}\ \text{Pr}\{E_2\}\ \text{Pr}\{E_3\}
\ \ \ \text{ for independent events}
\end{equation}

In general, if \(E_1\), \(E_2\), \(E_3\), \(\ldots\), \(E_n\) are \(n\)
independent events having respective probabilities \(p_1\), \(p_2\),
\(p_3\), \(\ldots\), \(p_n\), then the probability of occurrence of
\(E_1\) and \(E_2\) and \(E_3\) and \(\cdots E_n\) is
\(p_1 p_2 p_3 \cdots p_n\).

\textbf{Example 3.} Let \(E_1\) and \(E_2\) be the events ``heads on
fifth toss'' and ``heads on sixth toss'' of a coin, respectively. Then
\(E_1\) and \(E_2\) are independent events, and thus the probability of
heads on both the fifth and sixth tosses is (assuming the coin to be
fair)

\[
\text{Pr}\{E_1 E_2\} = \text{Pr}\{E_1\}\ \text{Pr}\{E_2\}
  = \left(\frac{1}{2}\right) \left( \frac{1}{2} \right) = \frac{1}{4}
\]

\textbf{Example 4.} If the probability that \(A\) will be alive in 20
years is 0.7 and the probability that \(B\) will be alive in 20 years is
0.5, then the probability that they will both be alive in 20 years is
\((0.7)(0.5) = 0.35\).

\textbf{Example 5.} Suppose that a box contains 3 white balls and 2
black balls. Let \(E_1\) be the event ``first ball drawn is black'' and
\(E_2\) the event ``second ball drawn is black,'' where the balls are
not replaced after being drawn. Here \(E_1\) and \(E_2\) are dependent
events.

The probability that the first ball drawn is black is
\(\text{Pr}\{E_1\} = 2 / (3 + 2) = \frac{2}{5}\). The probability that
the second ball is drawn is black, given that the first ball drawn was
black, is \(\text{Pr}\{E_2|E_1\} = 1 / (3 + 1) = \frac{1}{4}\). Thus the
probability that both balls drawn are black is

\[
\text{Pr}\{E_1 E_2\} = \text{Pr}\{E_1\}\ \text{Pr}\{E_2|E_1\}
= \frac{2}{5} \cdot \frac{1}{4} = \frac{1}{10}
\]

\hypertarget{mutually-exclusive-events}{%
\section{Mutually exclusive events}\label{mutually-exclusive-events}}

Two or more events are called \emph{mutually exclusive} if the
occurrence of any one of them excludes the occurrences of the others.
Thus if \(E_1\) and \(E_2\) are mutually exclusive events, then
\(\text{Pr}\{E_1 E_2\} = 0\).

If \(E_1 + E_2\) denotes the event that ``either \(E_1\) or \(E_2\) or
both occur,'' then

\begin{equation}
\text{Pr}\{E_1 + E_2\} = \text{Pr}\{E_1\} + \text{Pr}\{E_2\}
  - \text{Pr}\{E_1 E_2\}
\end{equation}

In particular,

\begin{equation}
\text{Pr}\{E_1 + E_2\} = \text{Pr}\{E_1\} + \text{Pr}\{E_2\}
\ \ \ \text{ for mutually exclusive events}
\end{equation}

As an extension of this, if \(E_1\), \(E_2\), \(\ldots\), \(E_n\) are
\(n\) mutually exclusive events having respective probabilities of
occurrence \(p_1\), \(p_2\), \(\ldots\), \(p_n\), then the probability
of occurrence of either \(E_1\) or \(E_2\) or \(\cdots E_n\) is
\(p_1 + p_2 + \cdots + p_n\).

Result (5) can also be generalized to three or more mutually exclusive
events.

\textbf{Example 6.} If \(E_1\) is the event ``drawing an ace from a deck
of cards'' and \(E_2\) is the event ``drawing a king,'' then
\(\text{Pr}\{E_1\} = \frac{4}{52} = \frac{1}{13}\) and
\(\text{Pr}\{E_2\} = \frac{4}{52} = \frac{1}{13}\). The probability of
drawing either an ace or a king in a single draw is

\[
\text{Pr}\{E_1 + E_2\} = \text{Pr}\{E_1\} + \text{Pr}\{E_2\}
  = \frac{1}{13} + \frac{1}{13} = \frac{2}{13}
\]

since both an ace and a king cannot be drawn in a single draw and are
thus mutually exclusive events (Fig. 6-1).

\textless\textless Fig. 6-1\textgreater\textgreater{}
\textless\textless Caption: \textbf{Fig. 6-1} \(E_1\) is the event
``drawing an ace'' and \(E_2\) is the event ``drawing a
king.''\textgreater\textgreater{}

Note that \(E_1\) and \(E_2\) have no outcomes in common. They are
mutually exclusive.

\textbf{Example 7.} If \(E_1\) is the event ``drawing an ace'' from a
deck of cards and \(E_2\) is the event ``drawing a spade,'' then \(E_1\)
and \(E_2\) are not mutually exclusive since the ace of spades can be
drawn (Fig. 6-2). Thus the probability of drawing either an ace or a
spade or both is

\[
\text{Pr}\{E_1 + E_2 \} = \text{Pr}\{E_1\} + \text{Pr}\{E_2\}
  - \text{Pr}\{E_1 E_2\} = \frac{4}{52} + \frac{13}{52} - \frac{1}{52}
  = \frac{16}{52} = \frac{4}{13}
\]

\textless\textless Fig. 6-2\textgreater\textgreater{}
\textless\textless Caption: \textbf{Fig. 6-2} \(E_1\) is the event
``drawing an ace'' and \(E_2\) is the event ``drawing a
spade.''\textgreater\textgreater{}

Note that the event ``\(E_1\) and \(E_2\)'' consisting of those outcomes
in both events is the ace of spades.

\hypertarget{probability-distributions}{%
\section{Probability distributions}\label{probability-distributions}}

\hypertarget{discrete}{%
\subsection{Discrete}\label{discrete}}

If a variable \(X\) can assume a discrete set of values
\(X_1, X_2, \ldots, X_K\) with respective probabilities
\(p_1, p_2, \ldots, p_K\), where \(p_1 + p_2 + \cdots + p_K = 1\), we
say that a \emph{discrete probability distribution} for \(X\) has been
defined. The function \(p(X)\), which has the respective values
\(p_1, p_2, \ldots, p_K\) for \(X = X_1, X_2, \ldots, X_K\), is called
the \emph{probability function}, or \emph{frequency function}, of \(X\).
Because \(X\) can assume certain values with given probabilities, it is
often called a \emph{discrete random variable}. A random variable is
also known as a \emph{chance variable} or \emph{stochastic variable}.

\textbf{Example 8.} Let a pair of fair dice be tossed and let \(X\)
denote the sum of the points obtained. Then the probability distribution
is as shown in Table 6.1. For example, the probability of getting sum 5
is \(\frac{4}{36} = \frac{1}{9}\); thus in 900 tosses of the dice we
would expect 100 tosses to give the sum 5.

Note that this is analogous to a relative-frequency distribution with
probabilities replacing the relative frequencies. Thus we can think of
probability distributions as theoretical or ideal limiting forms of
relative-frequency distributions when the number of observations made is
very large. For this reason, we can think of probability distributions
as being distributions of \emph{populations}, whereas relative-frequency
distributions are distributions of \emph{samples} drawn from this
population.

The probability distribution can be represented graphically by plotting
\(p(X)\) against \(X\), just as for relative-frequency distributions
(see Problem 6.11).

By cumulating probabilities, we obtain \emph{cumulative probability
distributions}, which are analogous to cumulative relative-frequency
distributions. The function associated with this distribution is
sometimes called a \emph{distribution function}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}@{}}
\caption{probability distribution of a pair of dice}\tabularnewline
\toprule\noalign{}
\endfirsthead
\endhead
\bottomrule\noalign{}
\endlastfoot
X & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
p(X) & 1/36 & 2/36 & 3/36 & 4/36 & 5/36 & 6/36 & 5/36 & 4/36 & 3/36 &
2/36 & 1/36 \\
\end{longtable}

\hypertarget{continuous}{%
\subsection{Continuous}\label{continuous}}

The above ideas can be extended to the case where the variable \(X\) may
assume a continuous set of values. The relative-frequency polygon of a
sample becomes, in the theoretical or limiting case of a population, a
continuous curve (such as in Fig. 6-3) whose equation is \(Y = p(X)\).
The total area under this curve bounded by the \(X\) axis is equal to 1,
and the area under the curve between lines \(X = a\) and \(X = b\)
(shaded in Fig. 6-3) gives the probability that \(X\) lies between \(a\)
and \(b\), which can be denoted by Pr\{\(a < X < b\)\}.

\textless\textless Fig. 6-3 Pr\{\(a < X < b\)\} is shown as the
cross-hatched area under the density function.\textgreater\textgreater{}

We call \(p(X)\) a \emph{probability density function}, or briefly a
\emph{density function}, and when such a function is given we say that a
\emph{continuous probability distribution} for \(X\) has been defined.
The variable \(X\) is then often called a \emph{continuous random
variable}.

As in the discrete case, we can define cumulative probability
distributions and the associated distribution functions.

\hypertarget{mathematical-expectation}{%
\section{Mathematical expectation}\label{mathematical-expectation}}

If \(p\) is the probability that a person will receive a sum of money
\(S\), the \emph{mathematical expectation} (or simply the
\emph{expectation}) is defined as \(pS\).

\textbf{Example 9.} Find \(E(X)\) for the distribution of the sum of the
dice given in Table 6.1. The distribution is given below. The
distribution is given by \texttt{p(X)}, the mathematical expectation is
given below the table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_6}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{12}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\SpecialCharTok{/}\DecValTok{36}\NormalTok{,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{1}\SpecialCharTok{/}\DecValTok{36}\NormalTok{), }\AttributeTok{nrow=}\DecValTok{2}\NormalTok{, }\AttributeTok{byrow=}\NormalTok{T)}
\NormalTok{output }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(table\_6}\FloatTok{.1}\NormalTok{, table\_6}\FloatTok{.1}\NormalTok{[}\DecValTok{1}\NormalTok{,]}\SpecialCharTok{*}\NormalTok{table\_6}\FloatTok{.1}\NormalTok{[}\DecValTok{2}\NormalTok{,]))}
\FunctionTok{colnames}\NormalTok{(output) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"p(X)"}\NormalTok{, }\StringTok{"XP(X)"}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(output)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrr@{}}
\toprule\noalign{}
X & p(X) & XP(X) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 & 0.0277778 & 0.0555556 \\
3 & 0.0555556 & 0.1666667 \\
4 & 0.0833333 & 0.3333333 \\
5 & 0.1111111 & 0.5555556 \\
6 & 0.1388889 & 0.8333333 \\
7 & 0.1666667 & 1.1666667 \\
8 & 0.1388889 & 1.1111111 \\
9 & 0.1111111 & 1.0000000 \\
10 & 0.0833333 & 0.8333333 \\
11 & 0.0555556 & 0.6111111 \\
12 & 0.0277778 & 0.3333333 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Mathematical expectation:"}\NormalTok{, }\FunctionTok{sum}\NormalTok{(output[,}\DecValTok{3}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Mathematical expectation: 7
\end{verbatim}

The concept of expectation is easily extended. If \(X\) denotes a
discrete random variable that can assume the values
\(X_1, X_2, \ldots, X_K\) with respective probabilities
\(p_1, p_2, \ldots, p_K\), where \(p_1 + p_2 + \cdots + p_K = 1\), the
\emph{mathematical expectation} of \(X\) (or simply the
\emph{expectation} of \(X\)), denoted by \(E(X)\), is defined as

\begin{equation}
E(X) = p_1 X_1 + p_2 X_2 + \cdots + p_k X_k = \sum^K_{j=1}{p_j X_j} = \sum{pX}
\end{equation}

If the probabilities \(p_j\) in this expectation are replaced with the
relative frequencies \(f_j/N\), where \(N = \sum{f_j}\), the expectation
reduces to \((\sum{fX})/N\), which is the arithmetic mean \(\bar{X}\) of
a sample of size \(N\) in which \(X_1, X_2, \ldots, X_K\) appear with
these relative frequencies. As \(N\) gets larger and larger, the
relative frequencies \(f_j/N\) approach the probabilities \(p_j\). Thus
we are led to the interpretation that \(E(X)\) represents the mean of
the population from which the sample is drawn. If we call \(m\) the
sample mean, we can denote the population mean by the corresponding
Greek letter \(\mu\) (mu).

Expectation can also be defined for continuous random variables, but the
definition requires the use of calculus.

\hypertarget{relation-between-population-sample-mean-and-variance}{%
\section{Relation between population, sample mean, and
variance}\label{relation-between-population-sample-mean-and-variance}}

If we select a sample of size \(N\) at random from a population (i.e.,
we assume that all such samples are equally probable), then it is
possible to show that the \emph{expected value of the sample mean m is
the population mean \(\mu\)}.

It does not follow, however, that the expected value of any quantity
computed from a sample is the corresponding population quantity. For
example, the expected value of the sample variance as we have defined it
is not the population variance, but \((N-1)/N\) times this variance.
This is why some staticians choose to define the sample variance as our
variance multiplied by \(N/(N-1)\).

\hypertarget{combinatorial-analysis}{%
\section{Combinatorial analysis}\label{combinatorial-analysis}}

In obtaining probabilities of complex events, an enumeration of cases is
often difficult, tedious, or both. To facilitate the labor involved, use
is made of basic principles studied in a subject called
\emph{combinatorial analysis}.

\hypertarget{fundamental-principle}{%
\subsection{Fundamental Principle}\label{fundamental-principle}}

If an event can happen in any one of \(n_1\) ways, and if when this has
occurred another event can happen in any one of \(n_2\) ways, then the
number of ways in which both events can happen in the specified order is
\(n_1 n_2\).

\textbf{Example 10.} The following is the function of the
\emph{factorial} for the numbers 0 through 5

\begin{figure}
\centering
\includegraphics{text_files/figure-latex/unnamed-chunk-3-1.pdf}
\caption{Chart for n!}
\end{figure}

\textbf{Example 11.} The number of permutations of the letters a, b, and
c taken two at a time is \(_{3}P_{2} = 3 \cdot 2 = 6\). These are ab,
ba, ac, ca, bc, and cb.

The number of permutations of \(n\) objects consisting of groups of
which \(n_1\) are alike, \(n_2\) are alike, \(\cdots\) is

\begin{equation}
\frac{n!}{n_1! n_2! \cdots}\ \ \ \ \text{where } n = n_1 + n_2 + \cdots
\end{equation}

\textbf{Example 12.} The number of permutations of letters in the word
\emph{statistics} is

\[
\frac{10!}{3! 3! 1! 2! 1!} = 50, 400
\]

since there are 3 \emph{s}'s, 3 \emph{t}'s, 1 \emph{a}, 2 \emph{i}'s,
and 1 \emph{c}.

\textbf{Combinations}

A combination of \(n\) different objects taken \(r\) at a time is a
selection of \(r\) out of the \(n\) objects, with no attention given to
the order of arrangement. The number of combinations of \(n\) objects
taken \(r\) at a time is denoted by the symbol \(\binom{n}{r}\) and is
given by

\begin{equation}
\binom{n}{r} = \frac{n (n - 1) \cdots (n - r + 1)}{r!} = \frac{n!}{r! (n-r)!}
\end{equation}

\textbf{Example 13.} The number of combinations of the letters a, b, and
c taken two at a time is

\[
\binom{3}{2} = \frac{3 \cdot 2}{2!} = 3
\]

\hypertarget{stirlings-approximation-to-n}{%
\section{Stirling's approximation to
n!}\label{stirlings-approximation-to-n}}

When \(n\) is large, a direct evaluation of \(n!\) is impractical. In
such cases, use is made of an approximate formula developed by James
Stirling:

\begin{equation}
n! \approx \sqrt{2 \pi n} n^n e^{-n}
\end{equation}

\end{document}
