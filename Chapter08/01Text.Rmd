---
title: 'Chapter 8: Elementary Sampling Theory'
subtitle: 'Text'
header-includes: \usepackage{amsmath}
output:
  pdf_document:
    toc: true
    toc_depth: '3'
---

# Sampling theory

_Sampling theory_ is a study of relationships existing between a population and samples drawn from the population. It is of great value in many connections. For example, it is useful in _estimating_ unknown population quantities (such as population mean and variance), often called _population parameters_ or briefly _parameters_, from a knowledge of corresponding sample quantities (such as sample mean and variance), often called _sample statistics_ or briefly _statistics_. Estimation problems are considered in Chapter 9.

Sampling theory is also useful indetermining whether the observed differences between two samples are due to chance variation or whether they are really significant. Such questions arise, for example, in testing a new serum for use in treatment of a disease or in deciding whether one production process is better than another. Their answers involve the use of so-called _tests of significance and hypotheses_ that are important in the _theory of decisions_. These are considered in Chapter 10.

In general, a study of the inferences made concerning a population by using samples drawn from it together with indications of the accuracy of such inferences by using probability theory, is called _statistical inference_.

# Random samples and random numbers

In order that the conclusions of sampling theory and statistical inference be valid, samples must be chosen so as to be _representative_ of a population. A study of sampling methods and of the related problems that arise is called the _design of the experiment_.

One way in which a representative smaple may be obtained is by a process called _random sampling_, according to which each member of a population has an equal chance of being included in the sample. One technique for obtaining a random sample is to assign numbers to each member of the population, write these numbers on small pieces of paper, place them in an urn, and then draw numbers from the urn, being careful to mix thoroughly before each drawing. An alternative method is to use a table of _random numbers_ (see Appendix IX) specially constructed for such purposes. See Problem 8.6.

# Sampling with and without replacement

If we draw a number from an urn, we have the choice of replacing or not replacing the number into the urn before a scond drawing. In the first case the number can come up again and again, whereas in teh second it can only come up once. Sampling where each member of the population may be chosen once it is called _sampling with replacement_, while if each member cannot be chosen more than once it is called _sampling without replacement_.

Populations are either finite or infinite. If, for example, we draw 10 balls successively without replacement from an urn containing 100 balls, we are sampling from a finite population, while if we toss a coin 50 times and count the number of heads, we are sampling from an infinite population.

A finite population in which sampling is with replacement can theoretically be considered infinite, since any number of samples can be drawn without exhausting the population. For many practical purposes, sampling from a finite population that is very large can be considered to be sampling from an infinite population.

# Sampling distributions

Consider all possible samples of size $N$ that can be drawn from a given population (either with or without replacement). For each sample, we can compute a statistic (such as the mean and the standard deviation) that will vary from sample to sample. In this manner we obtain a distribution of the statistic that is called its _sampling distribution_.

If, for example, the particular statistic used is the sample mean, then the distribution is called the _sampling distribution of the mean_. Similarly, we could have sampling distributions of standard deviations, variances, medians, proportions, etc.

For each sampling distribution, we can compute the mean, standard deviation, etc. Thus we can speak of the mean and standard deviation of the sampling distribution of means, etc.

# Sampling distribution of means

Suppose that all possible samples of size $N$ are drawn without replacement from a finite population of size $N_p > N$. If we denote the mean and standard deviation of the sampling distribution of means by $\mu_{\bar{X}}$ and $\sigma_{\bar{X}}$ and the population mean and standard deviation by $\mu$ and $\sigma$, respectively, then

\begin{equation}
\mu_{\bar{X}} = \mu\ \ \ \ \text{and}\ \ \ \ \sigma_{\bar{X}} =
\frac{\sigma}{\sqrt{N}} = \sqrt{\frac{N_p - N}{N_p - 1}}
\end{equation}

If the population is infinite or if sampling is with replacement, the above results reduce to

\begin{equation}
\mu_{\bar{X}} = \mu\ \ \ \ \text{and}\ \ \ \ \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{N}}
\end{equation}

FOr large values of $N$ $(N \ge 30)$, the sampling distribution of means is approximately a normal distribution with mean $\mu_{\bar{X}}$ and standard deviation $\sigma_{\bar{X}}$, irrespective of the population (so long as the population mean and variance are finite and the population size is at least twice the sample size). This result for an infinite population is a special case of the _central limit theorem_ of advanced probability theory, which shows that the accuracy of the approximation improves as $N$ gets larger. This is sometimes indicated by saying that the smapling distribution is _asymptotically normal_.

In case the population is normally distributed, the sampling distribution of means is also normally distributed even for small values of $N$ (i.e. $N < 30$).

# Sampling distribution of proportions

Suppose that a population is infinite and that the probability of occurrence of an event (called its success) is $p$, while the probability of nonoccurrence of the event is $q = 1 - p$. For example, the population may be all possible tosses of a fair coin in which the probability of the event "heads" is $p = \frac{1}{2}$. Consider all possible samples of size $N$ drawn from this population, and for each sample determine the proportion $P$ of successes. In the case of the coin, $P$ would be the proportion of heads turning up in $N$ tosses. We thus obtain a _sampling distribution of proportions_ whose mean $\mu_P$ and standard deviation $\sigma_P$ aer given by

\begin{equation}
\mu_P = p\ \ \ \ \text{and}\ \ \ \ \sigma_P = \sqrt{\frac{pq}{N}} = \sqrt{\frac{p(1-p)}{N}}
\end{equation}

which can be obtained from equations (2) by placing $\mu = p$ and $\sigma = \sqrt{pq}$. FOr large values of $N$ ($N \le 30$), the sampling distribution is very closely normally distributed. Note that the population is _binomially disributed_.

Equations (3) are also valid for a finit population in which sampling is with replacement. FOr finite populations in which sampling is without replacement, equations (3) are replaced by equations (1) with $\mu = p$ and $\sigma = \sqrt{pq}$.

Note that equations (3) are obtained most easily by dividing the mean and standard deviation ($N_P$ and $\sqrt{Npq}$) of the binomial distribution by $N$ (see Chapter 7).

# Sampling distribution of differences and sums

Suppose that we are given two populations. For each sample of size $N_1$ drawn from the first population, let us compute a statistic $S_1$; this yields a sampling distribution for the statistic $S_1$, whose mean and standard deviation we denote by $\mu_{S1}$ and $\sigma_{S1}$, respectively. Similarly, for each sample of size $N_2$ drawn from the second population, let us compute a statistic $S_2$; this yields a sampling distribution for the statistic $S_2$, whose mean and standard deviation are denoted by $\mu_{S2}$ and $\sigma_{S2}$. From all possible combinations of these samples from the two populations we can obtain a distribution of the differences, combinations of these samples from the two populations we can obtain a distribution of the differences, $S_1 - S_2$, which is called the _sampling distribution of differences of the statistics_. The mean and standard deviation of this sampling distribution, denoted respectively by $\mu_{S1 - S2}$ and $\sigma_{S1 - S2}$, are given by

\begin{equation}
\mu_{S1 - S2} = \mu_{S1} - \mu_{S2}\ \ \ \ \text{and}\ \ \ \ \sigma_{S1 - S2} = \sqrt{\sigma_{S1}^2 + \sigma_{S2}^2}
\end{equation}

provided that the samples chosen do not in any way depend on each other (i.e., the samples are _independent_).

If $S_1$, and $S_2$ are the sample means from the two populations--which means we denote by $\bar{X}_1$ and $\bar{X}_2$, respectively--then the sampling distribution of the differences of means is given for infinite populations with means and standard deviations $(\mu_1, \sigma_1)$ and $(\mu_2, \sigma_2)$, respectively, by

\begin{equation}
\mu_{\bar{X}_1 - \bar{X}_2} = \mu_{\bar{X}_1} - \mu_{\bar{X}_2} = \mu_1 - \mu_2
\ \ \ \ \text{and}\ \ \ \ \sigma_{\bar{X}_1 - \bar{X}_2} =
\sqrt{\sigma^2_{\bar{X}_1} + \sigma^2_{\bar{X}_2}} = \sqrt{\frac{\sigma^2_1}{N_1} +
\frac{\sigma_2^2}{N_2}}
\end{equation}

using equations (2). The result also holds for finite populations if sampling is with replacement. Similar results can be obtained from finite populations in which sampling is without replacement by using equations (1).

Corresponding results can be obtained for the sampling distributions of differences of proportions from two binomially distributed populations with parameters $(p_1, q_1)$ and $(p_2, q_2)$, respectively. In this case $S_1$ and $S_2$ correspond to the proportion of successes, $P_1$ and $P_2$, and equations (4) yield the results

\begin{equation}
\mu_{P1 - P2} = \mu_{P1} - \mu_{P2} = p_1 - P_2
\ \ \ \ \text{and}\ \ \ \ \sigma_{P1 - P2} = \sqrt{\sigma^2_{P1} + \sigma^2_{P2}} =
\sqrt{\frac{p_1 q_1}{N_1} + \frac{p_2 q_2}{N_2}}
\end{equation}

In $N_1$, and $N_2$ are large ($N_1$, $N_2 \ge 30$), the sampling distributions of differences of means or proportions are very closely normally distributed.

It is sometimes useful to speak of the _sampling distribution of the sum of statistics_. The mean and standard deviation of this distribution are given by

\begin{equation}
\mu_{S1 + S2} = \mu_{S1} + \mu_{S2}\ \ \ \ \text{and}\ \ \ \ \sigma_{S1 + S2} =
\sqrt{\sigma^2_{S1} + \sigma^2_{S2}}
\end{equation}

assuming that the samples are _independent_.

__Table 8.1 Standard Error for Sampling Distributions__

|Sampling Distribution|Standard Error|Special Remarks|
|:---:|:---:|:---:|
|Means|$\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{N}}$|This is true for large or small samples. The sampling distribution of means is very nearly normal for $N \ge 30$ even when the population is non-normal. $\mu_{\bar{X}} = \mu$, the population mean, in all cases.|
|Proportions|$\sigma_P = \sqrt{\frac{p(1 - p)}{N}} = \sqrt{\frac{pq}{N}}$| The remarks made for means apply here as well.  $\mu_{\bar{X}} = \mu$, the population mean, in all cases.|
|Standard deviations|$(1)\ \sigma_s = \frac{\sigma}{\sqrt{2N}}\ \ $ $\ \ (2)\ \sigma_s = \sqrt{\frac{\mu_4 - \mu_2^2}{4 N \mu_2}}$| For $N \ge 100$, the sampling distribution of $s$ is very nearly normal. $\sigma_s$ is given by (1) only if the population is normal (or approximately normal). If the population is nonnormal, (2) can be used.  Note that (2) reduces to (1) when $\mu_2 = \sigma^2$ and $\mu_4 = 3 \sigma^4$, which is true for normal populations.  For $N \ge 100$, $\mu_s = \sigma$ very nearly.|
|Medians|$$\sigma_{\text{med}} = \sigma \sqrt{\frac{\pi}{2N}} = \frac{1.2533 \sigma}{\sqrt{N}}$$| FOr $N \ge 30$, the sampling distribution of the median is very nearly normal. The given result holds only if the population is normal (or approximately normal).  $\mu_{\text{med}} = \mu$|
|First and thrid qurtiles|$$\sigma_{Q1} = \sigma_{Q3} = \frac{1.3626 \sigma}{\sqrt{N}}$$|The remarks made for medians apply here as well. $\mu_{Q1}$ and $\mu_{Q3}$ are very neraly equal to the first and third qurtiles of the population.  Note that $\sigma_{Q2} = \sigma{\text{med}}$|
|Deciles|$\sigma_{D1} = \sigma_{D9} = \frac{1.7094 \sigma}{\sqrt{N}}$ $\sigma_{D2} = \sigma_{D8} = \frac{1.4288 \sigma}{\sqrt{N}}$ $\sigma_{D3} = \sigma_{D7} = \frac{1.3180 \sigma}{\sqrt{N}}$ $\sigma_{D4} = \sigma_{D6} = \frac{1.2680 \sigma}{\sqrt{N}}$|The remarks made for medians apply hre as well. $\mu_{D1}, \mu_{D2},\ldots$ are very nearly equal to the first, second,... deciles of the population.  Note that $\sigma_{D5} = \sigma_{\text{med}}$|
|Semi-interquartile ranges|$\sigma_{Q} = \frac{0.7867 \sigma}{\sqrt{N}}$|The remarks made for medians apply here as well. $\mu_Q$ is very neraly equal to the population semi-interquartile range|
|Variances|$(1) \sigma_{S^2} = \sigma^2 \sqrt{\frac{2}{N}}$ $(2) \sigma_{S^2} = \sqrt{\frac{\mu_4 - \frac{N-3}{N-1} \mu_2^2}{N}}$|The remarks made for standard deviation apply here as well. Note that (2) yields (1) in the case that the population is normal. $\mu_{S^2} = \sigma^2 (N - 1)/N$, which is very nearly $\sigma^2$ for large $N$.|
|Coefficients of variation|$\sigma_V = \frac{v}{\sqrt{2N}} \sqrt{1 + 2v^2}$|Here $v = \sigma/\mu$ is the population coefficient of variation. THe given result holds for normal (or nearly normal) populations and $N \ge 100$.|

# Standard Errors

The standard deviation of a sampling distribution of a statistic is often called its _standard error_. Table 8.1 lists standard errors of sampling distributions for various statistics under the conditions of random sampling from an infinite (or very large) population or of sampling with replacement from a finite population. Also listed are special remarks giving conditions under which results are valid and other pertinent statements.

The quantites $\mu, \sigma, p, \mu_r$ and $\bar{X}, s, P, m_r$ denote, respectively, the population and sample means, standard deviations, proportions, and $r$th moments about the mean.

It is noted that if the sample size $N$ is large enough, the sampling distriubtion are normal or nearly normal. For this reason, the methods are known as _large sampling methods_. When $N < 30$, samples are called small. The theory of _small_ samples, or _exact sampling theory_ as it is sometimes called, is treated in Chapter 11.

When population parameters such as $\sigma, p,$ or $\mu_r$, are unknown, they may be estimated closely by their corresponding sample statitics namely, $s$ (or $\hat{s} = \sqrt{N/(N-1)}s$), $P$, and $m_r$--if the samples are large enough.

# Software are demonstration of elementary sampling theory

#### EXAMPLE 1.

A large population has the following random variable defined on it. $X$ represents the number of computers per household and $X$ is uniformly distributed, that is, $p(x) = 0.25$ for $x = 1, 2, 3,$ and $4$. In other words, $25\%$ of the households have 1 computer, $25\%$ have 2 computers, $25\%$ have 3 computers, and $25\%$ have 4 computers. The mean value of $X$ is $\mu = \sum{x p(x)} = 0.25 + 0.5 + 0.75 + 1 = 2.5$. The variance of $X$ is $\sigma^2 = \sum{x^2 p(x)} - \mu^2 = 0.25 + 1 + 2.25 + 4 - 6.25 = 1.25$. We say that the mean number of computers per household is 2.5 and the variance of the number of computers is 1.25 per household.

#### EXAMPLE 2.























